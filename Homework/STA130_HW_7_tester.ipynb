{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9aeff34",
   "metadata": {},
   "source": [
    "# Coding Homework 7: [Your Name]\n",
    "\n",
    "\n",
    "- You can add new cells if you need (with the \"+\" button above); but, deleting cells could very likely cause your notebook to fail MarkUs autotesting (and you'd have to start over and re-enter your answers into a completely fresh version of the notebook to get things to work again...)\n",
    "\n",
    "> TAs will mark this assignment by first checking ***MarkUs*** autotests for completion and general correctness, and then manually reviewing your written response to `Q12` and plotted figures for `Q17`\n",
    "> - The following questions \"automatically fail\" during automated testing so that MarkUs exposes example answers for student review and consideration for these problems.  These \"failed MarkUs tests\" are not counted against the student: `Q1`, `Q2`, `Q4`, `Q5`, `Q7`, `Q13`, and `Q18`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708502e",
   "metadata": {},
   "source": [
    "# Broadway!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5336c3",
   "metadata": {},
   "source": [
    "Lin-Manuel Miranda was nominated for “Best Original Song” for the March 27, 2022 the Academy Awards\n",
    "(also known as the Oscars) for his work on the Disney movie Encanto. Miranda had already won an Emmy,\n",
    "Grammy, and Tony (mostly for his work on the broadway musical “Hamilton”), so he was very close to\n",
    "the [EGOT](https://www.vanityfair.com/hollywood/2022/02/oscar-nominations-2022-will-lin-manuelmiranda-finally-egot-for-encanto) (Emmy, Grammy, Oscar and Tony), a rare occurrence as only 16 people\n",
    "have won all four awards. Unfortunately, Miranda did not win the Oscar in 2022. Perhaps he will soon!\n",
    "\n",
    "\n",
    "In this question we will look at a sample of weekly broadway musical data (available in `broadway.csv`).\n",
    "\n",
    "This data set contains a sample of Broadway musical information for 500 weeks from 1985 to 2020. In this\n",
    "data set an observation is one broadway musical in a particular week (ending on a Sunday). \n",
    "\n",
    "Variables of interest are:\n",
    "\n",
    "- show: Name of the broadway musical/show.\n",
    "- Hamilton: Indicates whether the musical is “Hamilton” or not.\n",
    "- week_ending: Date of the end of the weekly measurement period. Always a Sunday.\n",
    "- weekly_gross_overall: Weekly box office gross for all shows.\n",
    "- avg_ticket_price: Average price of tickets sold in a particular week.\n",
    "- top_ticket_price: Highest price of tickets sold in a particular week.\n",
    "- seats_sold: Total seats sold for all performances and previews in a particular week.\n",
    "- pct_capacity: Percent of theatre capacity sold. Shows can exceed 100% capacity by selling standing room tickets.\n",
    "\n",
    "Let’s explore different ways to estimate the average ticket price for Broadway shows! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eeff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import/Load Put the \"broadway.csv\" data with the name broadway\n",
    "# Take a look at the data to familiarize yourself with it and understand it a little more!\n",
    "\n",
    "\n",
    "broadway = None\n",
    "\n",
    "import pandas as pd\n",
    "broadway = pd.read_csv(\"broadway.csv\")\n",
    "\n",
    "broadway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f477d9",
   "metadata": {},
   "source": [
    "### Q0: Use `.drop(<columnName>, axis=1)` to remove any columns in `broadway` with missing values\n",
    "\n",
    "> - Hint 1: Use `.isna().any()` to check if any `columns` have missing values first, not rows!\n",
    "> - Hint 2: If you don't use `inplace=True` then you'll need to reassign the `broadway` object\n",
    "\n",
    "#### We wouldn't remove columns that we were interested in; but, then we'd probably remove the rows that have missing values in those columns.  It doesn't make sense to remove rows if the missing value is in a column we don't care about.  For the analyses here, then, what we're doing is removing coulmns that we won't need for our analyses instead of losing rows in columns that we're interested in for our analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23827a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q0: your answer will be tested by examining if your `broadway` object has been correctly updated!\n",
    "\n",
    "#broadway.isna().any()\n",
    "#broadway.drop('top_ticket_price', axis=1, inplace=True) # Can only do this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85c1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371a0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Ensure the dataframe name is broadway.\"\n",
    "hint += '''\n",
    "\n",
    "# We only need to remove top_ticket_price\n",
    "# [broadway[column].isnull().any() for column in broadway.columns] =\n",
    "# [False, False, False, False, False, True, False, False]\n",
    "# broadway = broadway.drop('top_ticket_price', axis=1)\n",
    "\n",
    "'''\n",
    "hint += \"Using just .drop() does not modify the dataframe: use the `inplace` parameter or reassign the data variable. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0207b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q0\n",
    "assert broadway.shape == (500, 7) and 'top_ticket_price' not in broadway, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d97fe",
   "metadata": {},
   "source": [
    "### Q1: Make a plot showing the relationship between `weekly_gross_overall` and `avg_ticket_price` on the x- and y-axis, respectively\n",
    "\n",
    "> - Hint: You did something like this in the final problem in the week three homework with the `scatter` function from `plotly.express`... *and of course there's always google...*\n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a421ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c42f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f098f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += '''\n",
    "import plotly.express as px\n",
    "fig = px.scatter(broadway, x=\"weekly_gross_overall\", y=\"avg_ticket_price\")\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q1\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1291d6",
   "metadata": {},
   "source": [
    "### Q2: Add a trendline to the plot\n",
    "\n",
    "\n",
    "#### Use the `trendline=\"ols\"` parameter argument for the `scatter` function\n",
    "\n",
    ">   This shows the fit of an $E[Y_i] = \\beta_0 + \\beta_1 x_i$ (\"intercept and slope\") simple linear model, specifically, \n",
    "> \n",
    ">   $$\\text{avg_ticket_price} = \\beta_0 + \\beta_1 \\text{weekly_gross_overall}$$  \n",
    ">\n",
    ">   that follows from the statistical modeling specification \n",
    "> \n",
    ">   $$Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\; \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07ed84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306ed99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += '''\n",
    "px.scatter(broadway, x=\"weekly_gross_overall\", y=\"avg_ticket_price\",\n",
    "           trendline=\"ols\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q2\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a0968",
   "metadata": {},
   "source": [
    "### Q3: Use `np.corrcoef` or the `.corr()` method for `pd.DataFrame` objects to compute the correlation between the variables in the Q2 plot above\n",
    "\n",
    "#### Supply your answer rounded to three decimal points with `np.round(answer, 3)` or using the `.round(3)` method\n",
    "\n",
    "- *Correlation measures of the strength of \"straight line\" linear association between two variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.corrcoef()\n",
    "broadway[['weekly_gross_overall','avg_ticket_price']].corr().iloc[0,1].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ecfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: your answer will be tested!\n",
    "Q3 = None # Provde 3 digits of accuracy as given by `np.round(answer, 3)` or the `.round(3)` method\n",
    "# E.g., Q3 = 0.123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadway[['weekly_gross_overall','avg_ticket_price']].corr().iloc[0,1].round(3)\n",
    "#np.round(np.corrcoef(broadway.weekly_gross_overall, broadway.avg_ticket_price)[0,1], 3)\n",
    "hint = '''\n",
    "broadway[[...]].corr().iloc[0,1].round(3)\n",
    "np.round(np.corrcoef(...,...)[0,1], 3)\n",
    "'''\n",
    "test = Q3 == 0.749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3705fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q3\n",
    "assert test, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75d0bc",
   "metadata": {},
   "source": [
    "### Q4: Explain why you do or do not think correlation provides a reasonable summary of the relationship between the variables in the plot(s) above? \n",
    "\n",
    "#### Correlation measures of the strength of \"straight line\" linear association between two variables\n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70caffa",
   "metadata": {},
   "source": [
    "> Answer here... \n",
    "\n",
    "Solution Elements:\n",
    "\n",
    "- Perhaps correlation is not the best measure of this relationsip because even though there does seem to be a straight line association in some sense, the association is stronger on the left than on the right, and this cannot be reflected in a correlation statistic summarizing the strength of a linear association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"Perhaps correlation is not the best measure of this relationsip because even though there does seem to be a straight line association in some sense, the association is stronger on the left than on the right, and this cannot be precicely reflected in a correlation statistic summarizing a single number measure of strength of a linear association\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q4\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc5897",
   "metadata": {},
   "source": [
    "# The Four Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd556154",
   "metadata": {},
   "source": [
    "In what we've done so far we're justing flirting with linear regression without knowing what important assumptions we could make to transform what we're doing into a statistical analysis. Understanding this will of course be highly relevant and immediately helpful for a lot of your future statistics courses, and as well entails many key statistical concepts that are just broadly useful in statistical analysis in general ways even beyond linear regrssion alone.\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i & \\sim  {} \\mathcal{N}\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2 \\right) &&\\text{simple linear regression model}\\\\\n",
    "Y_i & =  {} \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\; \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2 \\right) &&\\text{eqivalently expressed using errors terms } \\epsilon_i\\\\\n",
    "\\hat Y_i & =  {} \\hat \\beta_0 + \\hat \\beta_1 x_i &&\\text{predicted values of fitted model} \\\\\n",
    "\\hat \\epsilon_i & =  {} Y_i - \\hat Y_i &&\\text{residuals}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "## 1. Normality\n",
    "The $\\epsilon_i$ ***error terms*** of the model (which the $\\hat \\epsilon_i$ ***residuals*** sort of estimate and approximate) follow a normal distribution. We can check this using Q-Q plots or the Shapiro-Wilk test but for our purposes we can do a visual inspection. We can simply create a histogram of all the ***residuals*** and if it looks \"bell-shaped\" then we would likely be willing to believe that the assumption that the ***error terms*** follow a normal distribution is approximately true or at least relatively accurate.\n",
    "\n",
    "Here is an example, but of course in your specific context you would have to actually extract residuals based on a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03706419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "residuals = np.random.normal(0, 1, 1000)\n",
    "\n",
    "histogram = go.Histogram(x=residuals, nbinsx=30)\n",
    "layout = go.Layout(title='Histogram of Residuals', xaxis=dict(title='Residuals'), yaxis=dict(title='Frequency'))\n",
    "\n",
    "fig = go.Figure(data=[histogram], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa422b01",
   "metadata": {},
   "source": [
    "## 2. Homoscedasticity \n",
    "\n",
    "We have ***homoscedasticity*** if the $\\epsilon_i$ ***error terms*** of the model (which the $\\hat \\epsilon_i$ ***residuals*** sort of estimate and approximate) have a ***constant variance*** acorss every possible value of the $x_i$ measurements. If this condition is not met then you have ***heteroscedasticity*** which is not what we want (as opposed to ***homoscedasticity*** which is what we want). Higher ***variance*** in the ***error terms*** of the model makes it more difficult to trust our model, but this can be overcome by collecting more data. The simplest way to check ***homoscedasticity*** is to create a scattor plot for the fitted values against the residuals. If we see a cone-shaped plot, or some other systematic pattern in this plot that suggests the ***variance*** in the ***error terms*** is not constant across the $x_i$ values the the accuracy of our statistical analysis is going to suffer from the negative effects of ***heteroscedasticity***.\n",
    "\n",
    "Below we demonstrate \n",
    "- First what we're looking for and would consider reasonable support for an assumption of ***homoscedasticity***\n",
    "- Then an example of something we are not looking for and would consider evidence of ***heteroscedasticity***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "x = np.random.uniform(0, 1, n)\n",
    "y = 1 + 10*x + np.random.normal(0, 0.1, n)\n",
    "predicted_y = 1 + 10 * x  \n",
    "residuals = y - predicted_y\n",
    "data = pd.DataFrame({'Predicted Values': predicted_y, 'Residuals': residuals})\n",
    "\n",
    "fig = px.scatter(data, x='Predicted Values', y='Residuals', trendline=None)\n",
    "fig.update_layout(title='Scatter Plot observing Homoscedasticity in Residuals (what we want)',\n",
    "                  xaxis_title='Predicted Values', yaxis_title='Residuals')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we are not looking for, notice the \"cone shape\" as the points continue to spread\n",
    "# Notice the scale of the axis in both plots!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "predicted_values = np.random.uniform(0, 1, n)\n",
    "residuals = np.random.normal(0, predicted_values**2, n)\n",
    "\n",
    "scatter_residuals = go.Scatter(x=predicted_values, y=residuals, mode='markers', name='Residuals')\n",
    "\n",
    "layout_residuals = go.Layout(title='Scatter Plot observing Heteroscedasticity in Residuals (not what we want)',\n",
    "                             xaxis=dict(title='Predicted Values'), yaxis=dict(title='Residuals'))\n",
    "\n",
    "fig_residuals = go.Figure(data=[scatter_residuals], layout=layout_residuals)\n",
    "fig_residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a82cd",
   "metadata": {},
   "source": [
    "## 3. The \"Linear Form\" (with $x_i$ having no mismeasurement)\n",
    "\n",
    "The relationship between the independent variable(s) $x_{i}$ and the dependent variable $Y_i$ is as stated in the given linear specification. In other words, the change in the dependent variable is directly proportional to the change in the independent variables as specified by the linear model equation. In the case of simple linear regression forms such as $E[Y_i]=\\beta_0+\\beta_1 x_i$ this would manifest as the presense of a straight-line association visibly observable in the data. \n",
    "\n",
    "The adjoining assumption that each $x_i$ is measured exactly means that $x_i$ would not have any intrinsic randomness if the measurement were to be re-taken.  In many circumstances this would be a very naive assupmtion; but, there's also situations where it's not very hard to accept.  A fully grown person should always have the same height and the same color eyes, generally speaking.  So as long as our ability to take a measurement is consistent, then an assumption that $x_i$ is exactly measured would be reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "X = np.random.normal(0, 1, n)\n",
    "error_terms_also_called_noise = np.random.normal(0, 5, n)\n",
    "Y = 1 + 10*X + error_terms_also_called_noise\n",
    "\n",
    "scatter_plot = go.Scatter(x=X, y=Y, mode='markers', name='Data Points')\n",
    "\n",
    "layout = go.Layout(title='Scatter Plot',\n",
    "                   xaxis=dict(title='Independent Variable'), yaxis=dict(title='Dependent Variable'))\n",
    "\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031db57",
   "metadata": {},
   "source": [
    "## 4. Independence\n",
    "\n",
    "The observations in the dataset are ***independent*** of each other if there is no relationship between the $Y_i$ outcome (or equivalently $\\epsilon_i$ ***error term***) values.  We do want there to be a pattern among consecutive data points because if one data point can predict another data point then how much information is really contained in that other data point? It's not really a \"full unit\" of information is it?  For the purposes of statistical analyses this situation would mean that we would think we would have more information in our data than we really have.  This would lead to ineffective models and inaccurate statistical analyses. This is not unique to this assumption though. Failure of the previous three assumptions would as well lead to ineffective models and inaccurate statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65341e02",
   "metadata": {},
   "source": [
    "### Q5: Describe the shape of the data visualzed in Q2 relative to the added `trendline=\"ols\"` straight-line association with respect to the regression assumptions of *normality of error terms* and *homoskedastic (constant) variance*, and what this means for the appropriateness of these assumptions for the data.\n",
    "\n",
    "#### Write your answer in 1 to 3 sentences.\n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*\n",
    "\n",
    "> - Hint 1: The regression assumptions of *normality of error terms* and *homoskedastic (constant) variance* are nicely illustrated below\n",
    "> ![](https://stats.libretexts.org/@api/deki/files/1599/imageedit_3_7796089347.png?revision=1)\n",
    "> - Hint 2: Consider the *normality of error terms* and *homoskedastic (constant) variance* concepts illustrated here relative to your Q2 plot\n",
    "> - Hint 3: Note carefully that your plot in Q2 is not a \"fitted values versus residuals\" plot (as examined in the discussion of *homoskedasticity* above; rather, it's a plot of the original \"x versus y\" data; still, the assumptions of *normality* and *homoskedasticity* can nonetheless still be observed and evaluated by considering the behavior of the spread of the data relative to the trend line; and, this is what this problem is asking you to asses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713ffb0",
   "metadata": {},
   "source": [
    "> Answer here... \n",
    "\n",
    "Solution Elements:\n",
    "- Variance doesn't look constant (due to the fan shape of the data cloud)\n",
    "- Normality probably isn't true (as it's sort of bimodal perhaps) relative to the sort of straight line trend seen the the data\n",
    "- This means the assumptions of the normality of error terms and the homoskedasticity of variance of the error terms are not very appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"Variance doesn't look constant (due to the fan shape of the data cloud). \"\n",
    "hint += \"Normality probably isn't true (as it's sort of bimodal perhaps due to the 'cloud artifact' cluster in the upper right) relative to the sort of straight line trend seen the the data. \"\n",
    "hint += \"This means the assumptions of the normality of error terms and the homoskedasticity of variance of the error terms are not very appropriate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da504cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q5\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a9bad",
   "metadata": {},
   "source": [
    "### Q6: Add the following two new variable transformation columns to the `broadway` `pd.DataFrame` \n",
    "\n",
    "- `'log_avg_ticket_price'` which will be `np.log(broadway.avg_ticket_price)`\n",
    "- `'weekly_gross_overall_in_100k'` which will be `broadway.weekly_gross_overall/100000`\n",
    "\n",
    "#### The $\\log$ of a number less than or equal to zero is not defined so gets stored as `NaN`. Double check that there are no `NaN` values in the $\\log$-transformed columns and drop rows if there are any missing values resulting from the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadway['log_avg_ticket_price'] = np.log(broadway.avg_ticket_price)\n",
    "#broadway['weekly_gross_overall_in_100k'] = broadway.weekly_gross_overall/100000\n",
    "#broadway.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: your answer will be tested by examining if your `broadway` object has been correctly updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654b900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990489d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = '''Ensure only two new columns were added\n",
    "\n",
    "import numpy as np\n",
    "broadway[\"log_avg_ticket_price\"] = np.log(broadway.avg_ticket_price)\n",
    "broadway[\"weekly_gross_overall_in_100k\"] = broadway.weekly_gross_overall/100000\n",
    "\n",
    "# broadway['log_avg_ticket_price'].isnull().any() # No missing values in column\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q6\n",
    "assert broadway.shape == (500, 9) and 'log_avg_ticket_price' in broadway and 'weekly_gross_overall_in_100k' in broadway, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049be494",
   "metadata": {},
   "source": [
    "### Q7: Plot the association between the new variables you created along with a line of best fit to the plot, and describe the relationship observed in the plot, as well as any notable artifacts present in the plot.\n",
    "\n",
    "- `log_avg_ticket_price` (on the y-axis)\n",
    "- `weekly_gross_overall_in_100k` (on the x-axis)\n",
    "- Use the `trendline=\"ols\"` parameter argument for the `scatter` function\n",
    "\n",
    "#### Plot your figure in the code cell below and write your answer in 2 to 3 sentences.\n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f7d8dcc",
   "metadata": {},
   "source": [
    "> Answer here... \n",
    "\n",
    "Solution Elements:\n",
    "- The linear relationship has improved, there is less scattering on the right side of the graph\n",
    "- There is a clear outlier that has ~310 weekly gross overall income with the lowest log average ticket price\n",
    "- The homoskedastic (constant) variance assumption appears better, but the the artifact in the upper right suggests that the normality assumption may still not be appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"The linear relationship has improved in the sense that it seems more consistent: \"\n",
    "hint += \"there is less scattering on the right side of the graph. \"\n",
    "hint += \"The previously observed artifact in the plot is now transformed into a 'flat line of data' \"\n",
    "hint += \"at the top of the plot from about 250 to 400 on the x-axis \"\n",
    "hint += \"and leaves a strange 'hole' in the same range around the trend line. \"\n",
    "hint += \"It's also worth noting that there's a somewhat extreme outlier that at \"\n",
    "hint += \"~310 weekly gross overall income with the lowest log average ticket price. \"\n",
    "hint += \"The homoskedastic (constant) variance assumption appears better, but the the artifact in the upper right suggests that the normality assumption may still not be appropriate.\\n\"\n",
    "\n",
    "hint += \"\\n\\nCODE:\\n\"\n",
    "hint += '''\n",
    "fig = px.scatter(broadway, x=\"weekly_gross_overall_in_100k\", \n",
    "                 y=\"log_avg_ticket_price\", trendline=\"ols\")\n",
    "fig.show()'''\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q7\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddc7e0",
   "metadata": {},
   "source": [
    "### Q8: From your plot in Q7, which of the four assumptions of linear regression has improved the `most`?\n",
    "\n",
    "A. `Normality`  \n",
    "B. `Homoscedasticity`  \n",
    "C. `Linearity`  \n",
    "D. `Independence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: your answer will be tested!\n",
    "Q8 = None # Assign either 'A', 'B', 'C', or 'D' to `Q8` instead of `None`\n",
    "# E.g., Q8 = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ff514",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Do you see why? Consider what has happend to the variance of the points over the x-axis\"\n",
    "test = Q8==\"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d88480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q8\n",
    "assert test, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431aaa33",
   "metadata": {},
   "source": [
    "### Q9: Use `np.corrcoef` or the `.corr()` method for `pd.DataFrame` objects to compute the correlation between the variables in the Q7 plot above\n",
    "\n",
    "#### Supply your answer rounded to three decimal points with `np.round(answer, 3)` or using the `.round(3)` method\n",
    "\n",
    "- *Correlation measures of the strength of \"straight line\" linear association between two variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ee841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9 = broadway[['weekly_gross_overall_in_100k','log_avg_ticket_price']].corr().iloc[0,1].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank cell may or may not be overwritten... not sure about id so made test in fresh cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d75316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank cell may or may not be overwritten... not sure about id so made test in fresh cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea82722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadway[['weekly_gross_overall_in_100k','log_avg_ticket_price']].corr().iloc[0,1].round(3)\n",
    "#is equal to 0 or notnp.round(np.corrcoef(broadway[\"weekly_gross_overall_in_100k\"], broadway[\"log_avg_ticket_price\"]), 3)\n",
    "hint = '''\n",
    "broadway[[...]].corr().iloc[0,1].round(3)\n",
    "np.round(np.corrcoef(...,...)[0,1], 3)\n",
    "'''\n",
    "test = Q9 == 0.815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b648c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q9\n",
    "assert test, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168f4cc",
   "metadata": {},
   "source": [
    "### Q10: Which statement most accurately describes the change in correlations you found in Q3 and Q9?\n",
    "\n",
    "A. The correlation for Q9 has decreased significantly as more points are tightly fit to the line in Q3.    \n",
    "B. The correlation is similar because both plots look almost identical.  \n",
    "C. The correlation in Q9 is significantly larger as the points are more linear than in Q3.  \n",
    "D. The correlation is actually fairly similar despite the points in Q3 being both more and less tightly located around the line than the points in Q9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10: your answer will be tested!\n",
    "Q10 = None # Assign either 'A', 'B', 'C', or 'D' to `Q10` instead of `None`\n",
    "# E.g., Q10 = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"The correlations are not that different now; but, the relationship in the data is more 'consistent', no?\"\n",
    "test = Q10==\"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q10\n",
    "assert test, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493690ff",
   "metadata": {},
   "source": [
    "### Q11: Which of the following is the best statement? \n",
    "\n",
    "> - Hint: **Correlation measures of the strength of \"straight line\" linear association between two variables**; so, what does that mean when the strength of association is not consistent...?\n",
    "\n",
    "- A. The correlation measure is less useful for Q3 than Q9 since Q3 data has a more curved shaped\n",
    "\n",
    "\n",
    "- B. The correlation measure is less useful for Q3 than Q9 since Q3 data is more fan shaped  \n",
    "\n",
    "\n",
    "- C. The correlation measure is more useful for Q9 than Q3 since the linear association characterized in Q9 is more consistent compared to variably stronger and weaker association present in Q3  \n",
    "\n",
    "\n",
    "- D. Niether of the correlation measures from Q3 and Q9 are at all useful    \n",
    "\n",
    "\n",
    "- E. There's nothing different about correlation measures for Q3 and Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11: your answer will be tested!\n",
    "Q11 = None # Assign either 'A', 'B', 'C', 'D', or 'E' to `Q11` instead of `None`\n",
    "# E.g., Q11 = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q11\n",
    "assert Q11 == \"C\", \"Again, how has the average spread changed between both plots?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d30349",
   "metadata": {},
   "source": [
    "### Q12: For explanatory variable `weekly_gross_overall_in_100k` and response `log(avg_ticket_price)`, write down a simple linear regression model specification and explain each component of the model.\n",
    "\n",
    "#### Only writing down and interpreting the linear form is a sufficient response for this problem; but, if you are additionally welcome and encouraged to include some commentary consideration of the *error terms* as well if you wish to do so\n",
    "\n",
    "- *You may have noted the \"$E[Y_i] = \\beta_0 + \\beta_1 x_i$\" style notation as you've progressed through this assignment. This is the simple linear regression form, and is the part of the model that concerns itself with the average (where the $E$ is for \"expected\") behavior of the association of the data that is to be represented by the model. This distinguishes this aspect of the model from the full form of the statistical model which would include the error terms $\\epsilon_i$ as $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ and potentially even as well clearly specify the distribution of the error terms as $\\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)$.*\n",
    "\n",
    "> - Hint 1: \"The intercept of the line is XYZ and for a one unit increase in ZYX there is a XZY increase in ZXY\"\n",
    "> - Hint 2: You can repurpose and edit the comment in Q2 to write this out\n",
    "> - Hint 3: Double click on this cell to see how to write $\\log(\\text{text})$\n",
    "\n",
    "- *Your answer to this question will be manually reviewed by your TA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def518d0",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n",
    "Solution Elements:\n",
    "- $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x $\n",
    "- $\\hat{y}$ is the predicted value of log average ticket price for a given value of weekly gross overall\n",
    "- $\\hat{\\beta_0}$ is the intercept, does not make sense in this context\n",
    "- $\\hat{\\beta_1}$ is the average change in log average ticket price for 1-unit change in weekly gross overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eb6a1",
   "metadata": {},
   "source": [
    "![Scene from \"Return of the Jedi\"](https://cdn.vox-cdn.com/thumbor/q_ZnRdNwJWW1jaaT4tDEbYjonKY=/0x0:3840x2160/1200x800/filters:focal(1486x155:2100x769)/cdn.vox-cdn.com/uploads/chorus_image/image/65909572/scale.0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf5c96",
   "metadata": {},
   "source": [
    "# Return of the Hypothesis Test!\n",
    "\n",
    "> In a galaxy not very far away at all...\n",
    "\n",
    "Thinking back to the Amazon data from the lectures and recordings, suppose we wanted to see if the Amazon list price is associated with the Amazon sale price. One way to think about this is to check if the slope in a simple linear regression model with these two variables is zero or not. A zero slope would indicate that the an increase or decrease in list price does not associated with the sale price of the book. If the slope was not zero then a change in the list price does indeed seem to be associated with the sale price.\n",
    "\n",
    "To formalize this as a hypothesis test is to check if $\\beta_1$ (the slope of a simple linear regression model) is equal to zero or not.\n",
    "\n",
    "\\begin{align*}\n",
    "H_0: {}& \\beta_1 = 0 \\quad \\text{there is no \"$E$[sale_price] = $\\beta_0 + \\beta_1$list_price\" linear association between list and sale price}\\\\\n",
    "H_A: {}& \\beta_1 \\neq 0 \\quad \\text{there is some \"$E$[sale_price] = $\\beta_0 + \\beta_1$list_price\" linear association between these variables}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab4ab4",
   "metadata": {},
   "source": [
    "### Q13: For the model where `weekly_gross_overall_in_100k` predicts `log_avg_ticket_price` in a simple linear $\\beta_0 + \\beta_1x_i$ form, state the null and alternative hypotheses you would use to assess  the slope of the linear regression \n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fc256",
   "metadata": {},
   "source": [
    "> Answer here... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31811d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += '''\n",
    "- $H_0: \\\\beta_1 = 0$\n",
    "- $H_A: \\\\beta_1 \\\\neq 0$\n",
    "- We want to test whether the weekly gross overall income (in 100k) has any effect on log average ticket price by checking if the slope $\\\\beta_1$ is equal to 0 or not\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58615b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q13\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b42c22",
   "metadata": {},
   "source": [
    "### Q14:  Report and interpret the line equation of the ***OLS*** linear model fit from the `statsmodels` package by replacing the intercept $\\beta_0$ and slope $\\beta_1$ with their numeric values in the $\\beta_0 + \\beta_1 x_i$ equation and interpret the formula in the context of the data\n",
    "\n",
    "#### Report your numeric answers with 3 digits of accuracy using, e.g., `np.round(0.12345, 3)`\n",
    "\n",
    "- *This is actually in fact the same `trendline=\"ols\"` straight-line fit that you added to the plot in Q7 above*\n",
    "\n",
    "> - Hint 1: \"The intercept of the line is XYZ and for a one unit increase in ZYX there is a XZY increase in ZXY\"\n",
    "> - Hint 2: The functionality you should use is detailed in the [`statsmodels` documentation](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html)\n",
    "> - Hint 3: Intercepts must be manually added to covariates `X` (just once) using `sm.add_constant(X)`\n",
    "> - Hint 4: The term endogenous variable comes from econometrics and is synonymous with dependent, response, and outcome variable; and, similarly, the term exogenous variable is synonymous with covariate, feature, explanatory, and predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0292a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "263a848e",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14: your answer will be tested!\n",
    "\n",
    "intercept = None # Provde 3 digits of accuracy using `np.round(..., 3)`\n",
    "slope = None # Provde 3 digits of accuracy using `np.round(..., 3)`\n",
    "Q14 = (intercept, slope)\n",
    "\n",
    "# E.g., your final answer will be a tuple like Q14 = (0.123, 4.567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Look at the `sm.OLS(Y,X).fit().summary()` output table for the appropriate values.\\n\"\n",
    "hint += '''\n",
    "import statsmodels.api as sm\n",
    "Y = broadway.log_avg_ticket_price\n",
    "X = broadway.weekly_gross_overall_in_100k\n",
    "X = sm.add_constant(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a03019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q14\n",
    "assert Q14 == (3.238, 0.006), hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b79fec",
   "metadata": {},
   "source": [
    "### Q15: Make a conclusion about the hypotheses you defined above.\n",
    "\n",
    "#### Report the p-value for the slope coefficient and choose the correct interpretation from the options below\n",
    "\n",
    "A. p-value less than 0.001: very strong evidence against the null hypothesis  \n",
    "B. p-value less than 0.01 and greater than 0.001: strong evidence against the null hypothesis  \n",
    "C. p-value less than 0.05 and greater than 0.01: moderate/good evidence against the null hypothesis   \n",
    "D. p-value less than 0.10 and greater than 0.05: weak evidence against the null hypothesis    \n",
    "E. p-value greater than 0.10: no evidence against the null hypothesis    \n",
    "\n",
    "> - Hint: A OLS fit result in `statsmodels` has a `.summary()` method that provides information about the statistical analysis of the model fit as demonstrated in the [`statsmodels` documentation](https://www.statsmodels.org/stable/regression.html)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4a024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec774e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b321f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q15: your answer will be tested!\n",
    "\n",
    "p_value = None # Provide 3 digits of accuracy as given by the `.summary()` method\n",
    "MCA = None # Assign either 'A', 'B', 'C', 'D', or 'E' to `Q15` instead of `None`\n",
    "Q15 = (p_value, MCA)\n",
    "\n",
    "# E.g., your final answer will be a tuple like Q15 = (0.123,'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Look at the `sm.OLS(Y,X).fit().summary()` output table for the appropriate values.\\n\"\n",
    "hint += '''\n",
    "import statsmodels.api as sm\n",
    "Y = broadway.log_avg_ticket_price\n",
    "X = broadway.weekly_gross_overall_in_100k\n",
    "X = sm.add_constant(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q15\n",
    "assert Q15 == (0.000, 'A'), hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a95274",
   "metadata": {},
   "source": [
    "### Q16: Report the coefficient of determination $R^2$ for your model fit in Q14\n",
    "\n",
    "#### Provde 3 digits of accuracy in your answer as given by the `.summary()` method\n",
    "\n",
    "- *$R^2$ is the \"proportion of variance explained by the model\"*\n",
    "\n",
    "> - Hint: A linear model's $R^2 = r^2_{y\\hat y}$ where $r_{y\\hat y}$ is the correlation between the $y_i$'s the predicted $\\hat y_i$'s of the model, but this is also `.summary()` method available for OLS fit results in `statsmodels` as demonstrated in the [`statsmodels` documentation](https://www.statsmodels.org/stable/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13032cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aeb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q16: your answer will be tested!\n",
    "Q16 = None # Provde 3 digits of accuracy as given by the `.summary()` method\n",
    "# E.g., Q16 = 0.123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7087194",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Look at the `sm.OLS(Y,X).fit().summary()` output table for the appropriate values.\\n\"\n",
    "hint += '''\n",
    "import statsmodels.api as sm\n",
    "Y = broadway.log_avg_ticket_price\n",
    "X = broadway.weekly_gross_overall_in_100k\n",
    "X = sm.add_constant(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q16\n",
    "assert Q16 == 0.665, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb9663",
   "metadata": {},
   "source": [
    "# Introducing Categorical Variables\n",
    "\n",
    "In Q1 we created a plot to examine the assocation between weekly gross overall profit and average ticket price. One way to further study this relationship is to additionally consider other variables that might help us understand why the visualized data association looks the way it does.\n",
    "\n",
    "The two plots below show associations between the same data, but restricted to either when the show is not Hamilton or is Hamilton respectively. Titles are added for clarity on our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title needs to reflect boolean selection\n",
    "\n",
    "fig = px.scatter(broadway[broadway.Hamilton=='No'], x=\"weekly_gross_overall\", y=\"avg_ticket_price\", trendline=\"ols\")\n",
    "fig.update_layout(title=\"Weekly Gross Overall Income vs. Average Ticket Price - Show is not Hamilton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title needs to reflect boolean selection\n",
    "\n",
    "fig = px.scatter(broadway[broadway.Hamilton=='Yes'], x=\"weekly_gross_overall\", y=\"avg_ticket_price\", trendline=\"ols\")\n",
    "fig.update_layout(title=\"Weekly Gross Overall Income vs. Average Ticket Price - Show is Hamilton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295ddfa",
   "metadata": {},
   "source": [
    "### Q17: Add  `facet_col=\"Hamilton\"` and `color=\"Hamilton\"` to your plotting code from Q2 to examine these different associations in a more \"side by side\" manner\n",
    "\n",
    "#### Add titles to your figures summarizing the now better understood association between the `weekly_gross_overall` and `avg_ticket_price` variables with respect to whether or not a show was Hamilton \n",
    "\n",
    "> - Hint 1: Your code for Q2 should have been `px.scatter(broadway, x='weekly_gross_overall', y='avg_ticket_price', trendline=\"ols\")` and make sure to continue to use `trendline=\"ols\"` \n",
    "> - Hint 2: Don't use the boolean selections, like `[broadway.Hamilton=='Yes']` anymore\n",
    "> - Hint 3: Facet wraps allow you to view individual categories in their own graph and colouring allows you to colour a point based on the value of a categorial variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faceting\n",
    "\n",
    "fig = px.scatter(broadway, x=\"weekly_gross_overall\", y=\"avg_ticket_price\", facet_col=\"Hamilton\", trendline=\"ols\")\n",
    "fig.update_layout(title=\"Relation between weekly gross overall income and average ticket price faceted on Hamilton\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colouring\n",
    "\n",
    "fig = px.scatter(broadway, x=\"weekly_gross_overall\", y=\"avg_ticket_price\", color=\"Hamilton\", trendline=\"ols\")\n",
    "fig.update_layout(title=\"Relation between weekly gross overall income and average ticket price coloured on Hamilton\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9640b5",
   "metadata": {},
   "source": [
    "# Looking Forward...\n",
    "\n",
    "We will continue to explore the use of categorical variables in a linear model context next week, and additionally be ging to consider the simultaneously use multiple variables in linear model regression at once.  In the figure below, we demonstrate a version of this where indicators as well as a \"squared-transformed\" version of the `weekly_gross_overall` variable are all used in a linear regression model in order to produce the subsequent figure of the modeled association we are able to create under this (linear) specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Notice that we're no longer using `log_avg_ticket_price` and `weekly_gross_overall_in_100k`\n",
    "# as we've returned to using the original variables `avg_ticket_price` and `weekly_gross_overall`\n",
    "\n",
    "YX = broadway[['avg_ticket_price','weekly_gross_overall', 'Hamilton']].copy()\n",
    "YX['weekly_gross_overall_squared_Hamilton'] = (YX.Hamilton==\"Yes\")*YX.weekly_gross_overall**2\n",
    "model = smf.ols(formula='avg_ticket_price ~ 1 + weekly_gross_overall*Hamilton + weekly_gross_overall_squared_Hamilton', data=YX)\n",
    "fit_model = model.fit()\n",
    "\n",
    "fig = px.scatter(broadway, x='weekly_gross_overall', y='avg_ticket_price', color=\"Hamilton\")\n",
    "\n",
    "YX_subset_sorted = YX[broadway.Hamilton=='Yes'].sort_values('weekly_gross_overall')\n",
    "fig.add_trace(go.Scatter(x=YX_subset_sorted.weekly_gross_overall,\n",
    "                         y=fit_model.predict(YX_subset_sorted),\n",
    "                         mode=\"lines\", line=go.scatter.Line(color=\"red\"), showlegend=False))\n",
    "YX_subset_sorted = YX[broadway.Hamilton=='No'].sort_values('weekly_gross_overall')\n",
    "fig.add_trace(go.Scatter(x=YX_subset_sorted.weekly_gross_overall,\n",
    "                         y=fit_model.predict(YX_subset_sorted),\n",
    "                         mode=\"lines\", line=go.scatter.Line(color=\"blue\"), showlegend=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c52fb4",
   "metadata": {},
   "source": [
    "Very interestingly, even thought the modelled association between the `avg_ticket_price` and `weekly_gross_overall` variables for the shows that are `Hamilton` is now a curved relationship rather than a \"straight-line\" relationship, this curved relationship has indeed been created through a \"linear model specification\" by using a \"squared-transformed\" version of the `weekly_gross_overall` variable and including it into the model alongside the original `weekly_gross_overall` variable.  This is seen in the **multiple linear regression specification**.\n",
    "\n",
    "`formula='avg_ticket_price ~ 1 + weekly_gross_overall*Hamilton + weekly_gross_overall_squared_Hamilton'`\n",
    "\n",
    "which additionally uses the indicator variable `Hamilton` in an **interaction** specification alongside `weekly_gross_overall` which is what allows there to be two different prediction lines made by the same linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "\n",
    "hint += \"Correlation only characterizes linear assocaition in a meaningful way; so, \"\n",
    "hint += \"the fact that there is a curved relationship for 'Hamilton' shows means that \"\n",
    "hint += \"correlation may not be as appropriate for this data subset compared to non Hamilton shows \"\n",
    "hint += \"which do seem to have a little more a plain linaer relationship.\\n\\n\"\n",
    "\n",
    "hint += \"The normality homoscedasticity assumptions relative to each of the lines \"\n",
    "hint += \"appear as good as the other specifications we've looked at for this data. \"\n",
    "hint += \"A residual plot would suggest a bell shaped distribution, indicating \"\n",
    "hint += \"normality as a reasonable assumption; and, while there does appear to be \"\n",
    "hint += \"some difference in the variability of the data away from the lines at different \"\n",
    "hint += \"locations in the plot, it doesn't look 'too bad', relativley speaking \"\n",
    "hint += \"when each data subset is compared relative to it's own trend line.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q18\n",
    "assert False, hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c5dbd",
   "metadata": {},
   "source": [
    "### Q18: Consider the figure above and comment on (a) the appropriateness of using correlation as a measure to characterize the associations present in the two data subsets induced by whether a show is \"Hamilton\" or not, and (b) the appropriateness of the assumptions of *normality* and *homoskedastic (constant) variance* relative to the two prediction lines of this model.\n",
    "\n",
    "> - Hint 1: **Correlation measures of the strength of \"straight line\" linear association between two variables**\n",
    "> - Hint 2: This is indeed a single linear regression model; but, it's just able to make two different prediction lines due to the interaction with the `Hamilton` indicator variable\n",
    "> - Hint 3: As discussed above, this is still indeed technically a \"linear model\" even though it's able to produce a curved prediction line on the basis of the \"squared-transformed\" version of the `weekly_gross_overall` variable \n",
    "\n",
    "- *Compare your answer against the example answer provided in the **MarkUs** autotesting output*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbd18b",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
